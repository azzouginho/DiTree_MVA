# DiTree: Kinodynamic Motion Planning via Diffusion Trees (Guided Extension)

**Report:** More details here in `report.pdf`

> **MVA Project - Robotics**
> *Extension of the CoRL 2025 paper by Hassidof et al.*

---

## ðŸš€ Our Contribution: Guided Tree Search

While the original DiTree relies on global diffusion policies, exploring complex mazes with narrow passages remains challenging. In this fork, **we implemented a Guided Tree Search strategy** to bias the exploration.

### Key Features Implemented:
* **BFS Initialization:** We introduced a discrete grid-based BFS pre-processing step (`generate_intermediate_goals`) to find a coarse path skeleton avoiding static obstacles.
* **Intermediate Goals:** The planner now automatically generates a sequence of intermediate waypoints along the BFS path.
* **Biased Sampling Strategy:** Modified the `RRT.py` expansion loop. Instead of sampling purely randomly or towards the final goal, the tree now progressively targets the next intermediate goal based on a proximity cost heuristic (`get_distance_to_goal`).

**Result:** This approach significantly reduces the time to find a first solution in "Maze" environments by guiding the diffusion sampler through bottlenecks.

---

# DiTree

[project page](https://sites.google.com/view/ditree/home) , [paper](https://arxiv.org/abs/2508.21001)

Official implementation of the paper "Train Once Plan Anywhere Kinodynamic Motion Planning via Diffusion Trees" [CoRL 25]

This repository contains the code and experiment setup for  **Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees** . It includes training configuration, experiment execution scripts, and pretrained model checkpoints.

The associated car dataset and model weights are available for download [here](https://drive.google.com/drive/folders/1k8Btmcfqqa1YHKoZ8GNUEc8YKzK4SmWS?usp=drive_link).

---

## ðŸ“¦ Installation

We recommend creating a Python virtual environment before installing dependencies.

1. **Clone the repository**

```bash
git clone https://github.com/Yanivhass/ditree.git
cd ditree
```

2. **Create a virtual environment**

```bash
python3 -m venv venv
source venv/bin/activate   # On Linux/Mac
venv\Scripts\activate      # On Windows
```

3. **Install PyTorch**
   Visit [PyTorch.org](https://pytorch.org/get-started/locally/) to find the correct installation command for your system (CPU or GPU).
   Example (CUDA 11.8):

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

4. **Install other dependencies**
We provide here a minimal list of depencencies for running the car experiments. Other functionalities might require additional installation (e.g. Mujoco's Ant requiring Mujoco). 
```bash
pip install -r requirements.txt
```

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ run_scenarios.py      # Runs the experiments
â”œâ”€â”€ train_manager.py      # Configures and launches training sessions
â”œâ”€â”€ checkpoints/          # Contains pretrained model weights
â”œâ”€â”€ data/                 # (optional) Local dataset storage
â””â”€â”€ requirements.txt      # Python dependencies
```

---

## ðŸš€ Usage

### Running Experiments

```bash
python run_scenarios.py
```

### Training

```bash
python train_manager.py
```

---

## ðŸ“¥ Pretrained Models & Dataset

Pretrained model weights and the car dataset can be downloaded from:
[Google Drive Link](https://drive.google.com/drive/folders/1WiBU2g1qQn_2j6v1ZTB1eU0dAyCGoX7F?usp=sharing)
AntMaze dataset is available using Minari  [Minari]([https://pytorch.org/get-started/locally/](https://minari.farama.org/index.html))
Place the downloaded files into:

```
checkpoints/
data/
```

## Citation

If you use this work, please cite:

```bibtex
@inproceedings{
hassidof2025trainonce,
title={Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees},
author={Yaniv Hassidof and Tom Jurgenson and Kiril Solovey},
booktitle={9th Annual Conference on Robot Learning},
year={2025},
url={https://openreview.net/forum?id=lJWUourMTT}
}





